{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa4c717c",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "To use this for data preparation, \n",
    "- Prepare a single text file with all training materials.  \n",
    "- Remove as much unneccesary detail as possible (copyrights, tables of contents etc.) \n",
    "- Blank lines will be automatically deleted, but chapter names or other details will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b9c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from shared_project_functions import get_target_subdirectory\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intermediate functions\n",
    "\n",
    "def clean_sentences(corpus_name, dir, reset=False) -> list:\n",
    "    import re\n",
    "    \n",
    "    #load from pkl in subdirectory if exists\n",
    "    pkl_file = f\"{corpus_name}_cleaned_sentences.pkl\"\n",
    "    os_path = os.path.join(dir, pkl_file)\n",
    "    if not reset and os.path.exists(os.path.join(dir, pkl_file)):\n",
    "        with open(os.path.join(dir, pkl_file), \"rb\") as f:\n",
    "            cleaned_sentences = pickle.load(f)\n",
    "        print(f\"\\tLoaded cleaned sentences from {os.path.join(dir, pkl_file)}.\")\n",
    "    else:\n",
    "        # Create a new list to hold cleaned sentences\n",
    "        print(\"\\tNo cleaned sentences found - processing corpus...\")\n",
    "        cleaned_sentences = []\n",
    "        file_name = f\"{corpus_name}.txt\"\n",
    "        # Load corpus\n",
    "        try: # Load corpus from main directory\n",
    "            with open(os.path.join(file_name), \"r\") as f:\n",
    "                print(f\"\\tOpened {file_name} from main directory.\")\n",
    "                corpus = f.read()\n",
    "                move_file_flag = True\n",
    "        except FileNotFoundError:\n",
    "            print(f\"\\tCorpus file {file_name} not found in main directory - trying subdirectory.\")\n",
    "            try: # Load corpus from subdirectory dir\n",
    "                with open(os.path.join(dir, os.path.basename(file_name)), \"r\") as f:\n",
    "                    print(f\"\\tOpened {os.path.join(dir, os.path.basename(file_name))} from subdirectory.\")\n",
    "                    corpus = f.read()\n",
    "            except FileNotFoundError:\n",
    "                print(f\"\\tCorpus file {os.path.join(dir, os.path.basename(file_name))} not found in subdirectory either.\")\n",
    "                return []\n",
    "\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        nlp.max_length = len(corpus) + 10007    \n",
    "\n",
    "        print(f\"\\t\\tCorpus length (characters): {len(corpus)}\")\n",
    "        print(\"\\t\\tStarting spaCy sentence segmentation...\")\n",
    "        split_lines = corpus.split('\\n')  # or another delimiter\n",
    "        \n",
    "        #remove short lines such as blanks, \"Chapter 1\" etc.\n",
    "        split_lines = [line for line in split_lines if len(line.strip()) >= 3] \n",
    "        \n",
    "        print(f\"\\t\\tNumber of split lines: {len(split_lines)}\")\n",
    "        sentences = []\n",
    "        for doc in nlp.pipe(split_lines, batch_size=20, n_process=-1):\n",
    "            sentences.extend([sent.text.lower().strip() for sent in doc.sents])\n",
    "        print(\"\\t\\tFinished spaCy sentence segmentation.\")\n",
    "    \n",
    "        # Remove short sentences and clean\n",
    "        cleaned_sentences = []\n",
    "        for idx, s in enumerate(sentences):\n",
    "            words = [w for w in s.split() if not re.search(r'\\d', w)]\n",
    "            if len(words) > 1:\n",
    "                s_clean = ' '.join(words).lower()\n",
    "                cleaned_sentences.append(s_clean)\n",
    "\n",
    "        # Save cleaned sentences to pkl file in subdirectory dir\n",
    "        with open(os.path.join(dir, pkl_file), \"wb\") as f:\n",
    "            pickle.dump(cleaned_sentences, f)\n",
    "        print(f\"\\tSaved cleaned sentences to {os.path.join(dir, pkl_file)}.\")\n",
    "\n",
    "    return cleaned_sentences\n",
    "\n",
    "def process_lemmas(corpus_name, sentences, nlp, dir, reset=False):\n",
    "    pkl_file = f\"{corpus_name}_lemmas.pkl\"\n",
    "    \n",
    "    #load from pkl if exists\n",
    "    if not reset and os.path.exists(os.path.join(dir, pkl_file)):\n",
    "        with open(os.path.join(dir, pkl_file), \"rb\") as f:\n",
    "            all_lemmas = pickle.load(f)\n",
    "        print(f\"\\tLoaded lemmas from {os.path.join(dir, pkl_file)}.\")\n",
    "    else:\n",
    "        print(\"\\tProcessing lemmas...\")\n",
    "        processed_docs = nlp.pipe(sentences, batch_size=100, n_process=-1)\n",
    "        all_lemmas = []\n",
    "        for doc in processed_docs:\n",
    "            lemmas = [token.lemma_.lower() for token in doc if token.is_alpha]\n",
    "            all_lemmas.extend(lemmas)\n",
    "        \n",
    "        #save lemmas to pkl file in subdirectory dir\n",
    "        with open(os.path.join(dir, pkl_file), \"wb\") as f:\n",
    "            pickle.dump(all_lemmas, f)\n",
    "        print(f\"\\tSaved lemmas to {os.path.join(dir, pkl_file)}.\")\n",
    "    return all_lemmas\n",
    "\n",
    "def create_mappings(corpus_name, all_lemmas, dir, reset=False):\n",
    "    word_to_id_file = f\"{corpus_name}_word_to_id.json\"\n",
    "    id_to_word_file = f\"{corpus_name}_id_to_word.json\"\n",
    "    \n",
    "    #load from json files if exist\n",
    "    if not reset and os.path.exists(os.path.join(dir, word_to_id_file)) and os.path.exists(os.path.join(dir, id_to_word_file)):\n",
    "        with open(os.path.join(dir, word_to_id_file), \"r\") as f:\n",
    "            word_to_id = json.load(f)\n",
    "        with open(os.path.join(dir, id_to_word_file), \"r\") as f:\n",
    "            id_to_word = json.load(f)\n",
    "        print(f\"\\tLoaded mappings from {word_to_id_file} and {id_to_word_file}.\")\n",
    "    else:\n",
    "        special_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n",
    "        word_to_id = {token: i for i, token in enumerate(special_tokens)}\n",
    "        next_id = len(special_tokens)\n",
    "        for lemma in set(all_lemmas):\n",
    "            if lemma not in word_to_id:\n",
    "                word_to_id[lemma] = next_id\n",
    "                next_id += 1\n",
    "        id_to_word = {str(id): word for word, id in word_to_id.items()}\n",
    "        with open(os.path.join(dir, word_to_id_file), \"w\") as f:\n",
    "            json.dump(word_to_id, f)\n",
    "        with open(os.path.join(dir, id_to_word_file), \"w\") as f:\n",
    "            json.dump(id_to_word, f)\n",
    "        print(f\"\\tSaved mappings to {os.path.join(dir, word_to_id_file)} and {os.path.join(dir, id_to_word_file)}.\")\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def create_training_sequences(corpus_name, sentences, nlp, word_to_id, dir, max_seq_length=20, reset=False):\n",
    "    npz_file = f\"{corpus_name}_training_sequences.npz\"\n",
    "    \n",
    "    #load from file if exists\n",
    "    if not reset and os.path.exists(os.path.join(dir, npz_file)):\n",
    "        data = np.load(os.path.join(dir, npz_file))\n",
    "        X_train = data[\"X_train\"]\n",
    "        y_train = data[\"y_train\"]\n",
    "        print(f\"\\tLoaded training data from {os.path.join(dir, npz_file)}.\")\n",
    "    else:\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for sentence in sentences:\n",
    "            lemmatized_sentence = [token.lemma_.lower() for token in nlp(sentence) if token.is_alpha]\n",
    "            lemmatized_sentence = [\"<SOS>\"] + lemmatized_sentence + [\"<EOS>\"]\n",
    "            numerical_sentence = [word_to_id.get(word, word_to_id[\"<UNK>\"]) for word in lemmatized_sentence]\n",
    "            for i in range(1, len(numerical_sentence)):\n",
    "                input_seq = numerical_sentence[:i]\n",
    "                target_token = numerical_sentence[i]\n",
    "                if target_token == word_to_id[\"<UNK>\"]:\n",
    "                    continue  # Skip sequences with <UNK> target\n",
    "                 # Pad input_seq to max_seq_length\n",
    "                padded_input_seq = input_seq[:max_seq_length] + [word_to_id[\"<PAD>\"]] * (max_seq_length - len(input_seq))\n",
    "                X_train.append(padded_input_seq)\n",
    "                y_train.append(target_token)\n",
    "        X_train_np = np.array(X_train)\n",
    "        y_train_np = np.array(y_train)\n",
    "        #save to npz file in subdirectory dir\n",
    "        np.savez_compressed(os.path.join(dir, npz_file), X_train=X_train_np, y_train=y_train_np, max_seq_length=max_seq_length)\n",
    "        print(f\"\\tSaved training data to {os.path.join(dir, npz_file)}.\")\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94702b76",
   "metadata": {},
   "outputs": [],
   "source": [
    " #To do:\n",
    "# - Use Tokenizer from Tenensorflow rather than spacy for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae218e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: concatenate multiple text files into one corpus\n",
    "\n",
    "def concatenate_texts(directory, output_file_name):\n",
    "    import os\n",
    "    #concatenate all texts in directory\n",
    "    file_list = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".txt\")]\n",
    "    combined_text = \"\"\n",
    "    for file_name in file_list:\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "            combined_text += f.read() + \"\\n\"\n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(combined_text)\n",
    "    print(f\"Combined {len(file_list)} files into {output_file_name}.\")\n",
    "\n",
    "#concatenate_texts(\"model_1_sherlock/texts\", \"doyle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2043511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_name: str, \n",
    "               lemmatize: bool = False,\n",
    "               max_seq_length: int = 20,\n",
    "               reset: bool = False,\n",
    "               move_file_flag: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Preprocess the text corpus for training a language model.\n",
    "\n",
    "    Arguments:\n",
    "        file_name (str): Path to the corpus .txt file.\n",
    "        method (str): Preprocessing method. \"default\" uses spaCy and step-by-step processing.\n",
    "                      (Future methods can be added.)\n",
    "        max_seq_length (int): Maximum sequence length for training samples, needed by model.\n",
    "        reset (bool): If False, uses previously-saved intermediate files to save time.\n",
    "                      If True, starts over and overwrites any files.\n",
    "\n",
    "    Outputs:\n",
    "        Saves a single .pkl file with training data (X, y) and bidirectional mappings (word <-> id).\n",
    "\n",
    "    Examples:\n",
    "        - preprocess(\"shakespeare.txt\")\n",
    "        - preprocess(\"taylor_swift.txt\", reset=True)\n",
    "\n",
    "    To use pkl files in other notebooks:\n",
    "        ```python\n",
    "        with open(\"preprocessed_data.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            X_train = data[\"X_train\"]\n",
    "            y_train = data[\"y_train\"]\n",
    "            word_to_id = data[\"word_to_id\"]\n",
    "            id_to_word = data[\"id_to_word\"]\n",
    "            max_seq_length = data[\"max_seq_length\"]\n",
    "        ```\n",
    "    \"\"\"\n",
    "    import spacy\n",
    "    import pickle\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import json    \n",
    "    import re\n",
    "    \n",
    "    corpus_name = file_name.replace(\".txt\", \"\")\n",
    "    print(f\"Preprocessing {corpus_name}\")\n",
    "    dir = get_target_subdirectory(corpus_name)\n",
    "    \n",
    "    # Sentence cleaning\n",
    "    cleaned_sentences = clean_sentences(corpus_name, dir, reset=reset)\n",
    "\n",
    "    print(f\"\\t\\tTotal number of sentences in corpus: {len(cleaned_sentences)}\")\n",
    "    #example sentence\n",
    "    random_sentence_index = np.random.randint(0, len(cleaned_sentences))\n",
    "    print(f\"\\t\\tExample cleaned sentence [{random_sentence_index}]: {cleaned_sentences[random_sentence_index]}\")\n",
    "    \n",
    "    # Lemmatization\n",
    "    if lemmatize:\n",
    "        all_lemmas = process_lemmas(corpus_name, cleaned_sentences, nlp, dir, reset)\n",
    "        print(f\"\\t\\tTotal lemmas extracted: {len(all_lemmas)}\")\n",
    "    else:\n",
    "        print(\"\\tSkipping lemmatization\")\n",
    "\n",
    "    # Mapping\n",
    "    if lemmatize:\n",
    "        word_to_id, id_to_word = create_mappings(corpus_name, all_lemmas, dir, reset)\n",
    "    else:\n",
    "        # Flatten cleaned_sentences into a list of words\n",
    "        all_words = []\n",
    "        for s in cleaned_sentences:\n",
    "            all_words.extend([w for w in s.split() if w.isalpha()])\n",
    "        word_to_id, id_to_word = create_mappings(corpus_name, all_words, dir, reset)\n",
    "    print(f\"\\t\\tVocabulary size (including special tokens): {len(word_to_id)}\")\n",
    "\n",
    "    # Training Sequences\n",
    "    X_train, y_train = create_training_sequences(corpus_name, cleaned_sentences, nlp, word_to_id, dir, max_seq_length, reset=reset)\n",
    "    print(f\"\\t\\tNumber of training samples: {len(X_train)}\")\n",
    "\n",
    "    # Save all training data to pkl\n",
    "    data = {\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"word_to_id\": word_to_id,\n",
    "        \"id_to_word\": id_to_word,\n",
    "        \"max_seq_length\": max_seq_length\n",
    "    }\n",
    "    \n",
    "    # Save to pkl file in subdirectory dir\n",
    "    output_filename = f\"{corpus_name}_preprocessed_data.pkl\"\n",
    "    with open(os.path.join(dir, output_filename), \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"\\tSaved preprocessed data to {os.path.join(dir, output_filename)}\")\n",
    "    \n",
    "    # Move source file to subdirectory dir\n",
    "    if move_file_flag:\n",
    "        try:\n",
    "            os.rename(file_name, os.path.join(dir, os.path.basename(file_name)))\n",
    "            print(f\"\\tMoved {file_name} to {os.path.join(dir, os.path.basename(file_name))}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\tCould not move {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0671d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing doyle\n",
      "Corpus: doyle   Directory: model_2_doyle, reset: False\n",
      "pklfile: doyle_cleaned_sentences.pkl\n",
      "os_path: model_2_doyle/doyle_cleaned_sentences.pkl\n",
      "\tLoaded cleaned sentences from model_2_doyle/doyle_cleaned_sentences.pkl.\n",
      "\t\tTotal number of sentences in corpus: 189298\n",
      "\t\tExample cleaned sentence [41571]: i hope that you will continue your duties here until you have found a place elsewhere.â€\n",
      "\tSkipping lemmatization\n",
      "\tLoaded mappings from doyle_word_to_id.json and doyle_id_to_word.json.\n",
      "\t\tVocabulary size (including special tokens): 38177\n",
      "\tSaved training data to model_2_doyle/doyle_training_sequences.npz.\n",
      "\t\tNumber of training samples: 3672672\n",
      "\tSaved preprocessed data to model_2_doyle/doyle_preprocessed_data.pkl\n",
      "\tCould not move doyle.txt: [Errno 2] No such file or directory: 'doyle.txt' -> 'model_2_doyle/doyle.txt'\n"
     ]
    }
   ],
   "source": [
    "preprocess(\"doyle.txt\", reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a9118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-preprocessing (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
